Times:

10 simulations: 0m0.031s
100 simulations: 0m0.032s
1000 simulations: 0m0.041s
10000 simulations: 0m0.124s
100000 simulations: 0m0.958s
1000000 simulations: 0m9.595s

Questions:

Which predictions, if any, proved incorrect as you increased the number of simulations?:

With a small number of simulations, for the mens competition, the predictions were generally that Switzerland would be in the top 3 and France would be near the bottom of the list of potential winners however with more simulations Switzerland and France were both consistently shown near the middle of the predictions. The percentage probabilty for each team also varied and stabilised with increasing simulations with one of the biggest changes being Belgium's chances going from 40% with 10 simulations, to 24% with 100 simulations, then eventually stabilising to around 21% with 1000000 simulations.

Suppose you're charged a fee for each second of compute time your program uses.
After how many simulations would you call the predictions "good enough"?:

It seems like the predictions stabilized after about 100,000 simulations. So assuming the goal is to maximise accuracy for a reasonable cost, I would say 100,000 simulations is "good enough" because this runs in under a second and provides results that are very close, generally within 0.1%, to results with the higher 1,000,000 simulations which run in almost 10 seconds. So the 10x increase in compute costs does not affect the quality of the results a lot. Depending on the application and required accuracy, 10,000 simulations could also be good enough as they also provide a close deviation in results of around 0.6% (generally less than 1%) at most but result in a tenth of the cost.
